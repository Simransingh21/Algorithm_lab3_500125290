### **Design and Analysis of Algorithms Lab**

Welcome to the Design and Analysis of Algorithms Lab—where theoretical concepts meet real-world applications! This repository is your playground to explore, implement, and compare different algorithmic approaches across multiple domains. With a focus on performance, time complexity, and real-world optimization, you'll get hands-on experience with powerful problem-solving tools. Let's dive in!

---

### **Lab 1: Insertion in Binary Search Tree (BST)**

**Goal:**  
Implement and compare iterative and recursive methods for inserting elements into a Binary Search Tree (BST) and assess their performance.

**Analysis:**  
- **Iterative Approach:** Uses loops instead of recursion, usually more memory-efficient.  
- **Recursive Approach:** Conceptually simpler but comes with overhead from function calls.  
- **Time Complexity:** Both are O(log n) for balanced trees. However, recursive methods might add overhead due to multiple function calls.

---

### **Lab 2: Sorting Algorithms - Merge Sort vs. Quick Sort**

**Goal:**  
Implement and compare Merge Sort and Quick Sort on the same dataset and watch them battle it out for speed!

**Analysis:**  
- **Merge Sort:** Stable and reliable with O(n log n) performance, ideal for larger datasets but requires extra memory.  
- **Quick Sort:** Often faster for average cases, O(n log n) on average, but can hit a worst-case O(n²) if pivots aren’t chosen wisely.  
- **Stability:** Merge Sort is stable (keeps order intact), while Quick Sort isn’t.

---

### **Lab 3: Matrix Multiplication - Strassen’s vs. Traditional**

**Goal:**  
Compare the traditional matrix multiplication method with Strassen’s algorithm, showcasing the power of efficiency!

**Analysis:**  
- **Traditional Method:** O(n³) time complexity—perfectly fine for smaller matrices but starts slowing down for large ones.  
- **Strassen’s Algorithm:** O(n².81) complexity, faster for large matrices, but requires some extra steps.

---

### **Lab 4: Activity Selection Problem (Greedy Approach)**

**Goal:**  
Solve the Activity Selection Problem using a Greedy approach—maximize your activities without overlaps!

**Analysis:**  
- **Greedy Approach:** Selects the best option at each step, giving an optimal solution for this specific problem.  
- **Time Complexity:** O(n log n), primarily from sorting. This is lightning-fast for the right dataset!

---

### **Lab 5: Matrix Chain Multiplication (Dynamic Programming)**

**Goal:**  
Implement Matrix Chain Multiplication using Dynamic Programming to minimize the computational cost by storing intermediate results.

**Analysis:**  
- **Dynamic Programming Approach:** Breaks down the problem and avoids redundant calculations, with O(n³) time complexity.  
- **Efficiency:** Cuts down on time but can be memory-intensive. Memory management is key!

---

### **Lab 6: Shortest Path Algorithms – Dijkstra vs. Bellman-Ford**

**Goal:**  
Find the shortest path from a source node using Dijkstra and Bellman-Ford algorithms—what’s the fastest way to reach your destination?

**Analysis:**  
- **Dijkstra’s Algorithm:** Best for graphs with non-negative weights and usually O(V log V + E)—fast and efficient!  
- **Bellman-Ford Algorithm:** Works with negative weights and has a higher complexity of O(VE), but it’s like a Swiss army knife when negative weights appear.

---

### **Lab 7: 0/1 Knapsack Problem - Greedy vs. Dynamic Programming**

**Goal:**  
Solve the 0/1 Knapsack Problem using Greedy and Dynamic Programming, and compare the results!

**Analysis:**  
- **Greedy Approach:** Fast but may not always give the optimal solution.  
- **Dynamic Programming:** Provides the optimal solution but at the cost of more time and memory.  
- **Time Complexity:** Greedy is O(n log n), but Dynamic Programming has O(nW), where W is the weight capacity.

---

### **Lab 8: Subset Sum Problem (Recursive Approach)**

**Goal:**  
Solve the Subset Sum Problem using recursion to find subsets that sum to a target value. A great exercise for recursion lovers!

**Analysis:**  
- **Recursive Approach:** Can lead to exponential time complexity O(2^n), but it’s intuitive and great for learning recursion.  
- **Dynamic Programming:** Uses a table to store intermediate results, significantly improving efficiency.

---

### **Lab 9: 0/1 Knapsack Problem – Backtracking vs. Branch & Bound vs. DP**

**Goal:**  
Compare Backtracking, Branch & Bound, and Dynamic Programming to solve the 0/1 Knapsack Problem and determine the most efficient method.

**Analysis:**  
- **Backtracking:** Tries all possibilities, which can be slow but saves memory compared to DP.  
- **Branch & Bound:** Prunes branches and gives a good balance between speed and optimality.  
- **Dynamic Programming:** The optimal solution, but with high memory usage and O(nW) time complexity.

---

### **Lab 10: String Matching Algorithms – Rabin-Karp vs. KMP vs. Naive**

**Goal:**  
Implement Rabin-Karp, Knuth-Morris-Pratt (KMP), and Naive string-matching algorithms to find patterns in large texts.

**Analysis:**  
- **Naive Algorithm:** Brute-force approach with O(nm) time complexity. It's simple but slow.  
- **Rabin-Karp Algorithm:** O(n + m) on average using hashing, but it can degrade due to hash collisions.  
- **KMP Algorithm:** Efficient with O(n + m) complexity, and avoids the pitfalls of hash collisions.

---

### **Conclusion**

Congratulations on exploring these core algorithms! Each lab in this repository is designed to provide an engaging hands-on experience with critical algorithms used in computer science. By comparing different approaches—whether Greedy, Dynamic Programming, Backtracking, or Divide and Conquer—you’ll gain practical knowledge of their performance and trade-offs.  
Dive deeper, experiment with different datasets, and build a strong foundation in algorithm analysis!

---
